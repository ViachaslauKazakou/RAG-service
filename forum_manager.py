import hashlib
from typing import List, Dict, Optional, Union
import json
import re
from app.utils.logger_utils import timer, setup_logger
from app.ai_manager.rag_langchain import AdvancedRAG
from ollama import chat
from ollama import ChatResponse

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
logger = setup_logger(__name__)


class AIModels:
    """
    Class to manage AI models and their identifiers.
    """

    light: str = "google/flan-t5-base"
    medium: str = "google/flan-t5-large"
    heavy: str = "google/flan-t5-xl"
    light_1: str = "mistralai/Mistral-7B-Instruct-v0.2"
    distilgpt2: str = "distilgpt2"
    llama_mistral: str = "mistral:latest"
    deepseek: str = "deepseek-r1"
    gemma: str = "gemma3:latest"


class CharacterPersona:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ —Ñ–æ—Ä—É–º–∞"""

    CHARACTERS = {
        "alaev": {
            "type": "forum_troll",
            "personality": "–Ø–∑–≤–∏—Ç–µ–ª—å–Ω—ã–π –∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π, –≥—Ä—É–±—ã–π, –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å, —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞—Ä–∫–∞–∑–º, –ü–æ—Å—Ç–æ—è–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞",
            "speech_pattern": "–î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –æ—Ç–æ—Ä–≤–∞–Ω–Ω—ã–µ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –ª—é–±–∏—Ç –æ—Ç—Ä–∏—Ü–∞—Ç—å –≤—Å–µ –∏ —Å–ø–æ—Ä–∏—Ç—å —Å –æ–∫—Ä—É–∂–∞—é—â–∏–º–∏",
            "expertise": "–°—Ç–∞—Ä—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫—Ä–∏—Ç–∏–∫–∞ –Ω–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –Ω–µ–Ω–∞–≤–∏–¥–∏—Ç –ü–æ–ª—å—à—É",
            "mood_variations": ["sarcastic", "aggressive", "nostalgic", "provocative"],
        },
        "Sly32": {
            "type": "senior_python_engineer",
            "personality": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, —Ç–µ—Ä–ø–µ–ª–∏–≤—ã–π, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π",
            "speech_pattern": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è",
            "expertise": "Python, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, best practices",
            "mood_variations": ["helpful", "professional", "patient", "analytical"],
        },
        "Data_Scientist": {
            "type": "senior_data_scientist",
            "personality": "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –º–µ—Ç–æ–¥–∏—á–Ω—ã–π",
            "speech_pattern": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –≥—Ä–∞—Ñ–∏–∫–∏, –Ω–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥",
            "expertise": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
            "mood_variations": ["analytical", "curious", "methodical", "research_focused"],
        },
        "Domen77": {
            "type": "forum_troll",
            "personality": "–ó–∞–≤–∏—Å—Ç–ª–∏–≤—ã–π, –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π, –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å, –±–æ–ª—Ç–ª–∏–≤—ã–π –Ω–µ –ø–æ —Ç–µ–º–µ",
            "speech_pattern": "–û—Ç–≤–µ—á–∞–µ—Ç —á–∞—Å—Ç–æ —Å —Å–∞—Ä–∫–∞–∑–º–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–∏",
            "expertise": "–ù–µ—Ç –æ—Å–æ–±–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã, –ø—Ä–æ—Å—Ç–æ –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å, –ø—Ä–æ–≤–æ—Ü–∏—Ä–æ–≤–∞—Ç—å, –Ω–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–µ–Ω –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ –∫–æ—Ç–æ—Ä—ã—Ö –ø–∏—à–µ—Ç",
            "mood_variations": ["provocative", "sly", "encouraging", "neutral"],
        },
        "nechaos": {
            "type": "forum_troll",
            "personality": "–ó–∞–≤–∏—Å—Ç–ª–∏–≤—ã–π, –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–π, –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å",
            "speech_pattern": "–û—Ç–≤–µ—á–∞–µ—Ç —á–∞—Å—Ç–æ –Ω–µ–≤–ø–æ–ø–∞–¥, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–∏, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç—ã, –∏ –∂–∞—Ä–≥–æ–Ω, –≤—Å–µ–≥–¥–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –Ω–µ –±–æ–ª–µ–µ 100 –∑–Ω–∞–∫–æ–≤–ë –≤ –∫–æ–Ω—Ü–µ –ª—é–±–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—Ä–∞–∑—É '–ï—Å–ª–∏ –í–∞–º –≤—ã—É—á–∏—Ç—å—Å—è –Ω–∞ [–ø—Ä–æ—Ñ–µ—Å—Å–∏—è], —á—Ç–æ–±—ã –∫—Ä–∏—Ç–∏–∫–∞–Ω—ã –Ω–µ –º–æ–≥–ª–∏ –¥–∞–≤–∏—Ç—å –Ω–∞ —ç—Ç–æ —É—è–∑–≤–∏–º–æ–µ –º–µ—Å—Ç–æ?'",
            "expertise": "–≥–ª—É–ø—ã–π, –Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω—ã–π, –ª—é–±–∏—Ç —Å–ø–æ—Ä–∏—Ç—å, –ª—é–±–∏—Ç —Ñ–ª—É–¥–∏—Ç—å, –æ—Ç—Ä–∏—Ñ–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ò—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
            "mood_variations": ["provocative", "dumb", "encouraging", "neutral"],
        },
    }


class ForumRAG(AdvancedRAG):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ä—É–º–Ω—ã–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏"""

    def __init__(self, documents_path: str = "app/ai_forum/forum_knowledge_base", cache_path: str = "forum_cache"):
        super().__init__(documents_path, cache_path)
        self.character_persona = CharacterPersona()
        self.model = AIModels.gemma  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å Gemma3 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def parse_character_message(self, text: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏–∑ JSON –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å–æ–æ–±—â–µ–Ω–∏—è: {text[:100]}...")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        try:
            # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
            if text.strip().startswith("{") and text.strip().endswith("}"):
                logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON –æ–±—ä–µ–∫—Ç")
                data = json.loads(text)
                result = self._normalize_json_message(data)
                logger.info(f"   ‚úÖ JSON –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: character='{result.get('character')}', content='{result.get('content', '')[:50]}...'")
                return result

            # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ JSON –æ–±—ä–µ–∫—Ç–æ–≤
            if text.strip().startswith("[") and text.strip().endswith("]"):
                logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON –º–∞—Å—Å–∏–≤")
                data = json.loads(text)
                if isinstance(data, list) and len(data) > 0:
                    result = self._normalize_json_message(data[0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                    logger.info(f"   ‚úÖ JSON –º–∞—Å—Å–∏–≤ –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: character='{result.get('character')}', content='{result.get('content', '')[:50]}...'")
                    return result

            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–¥—Ä—è–¥
            logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞")
            json_objects = self._extract_json_objects(text)
            if json_objects:
                result = self._normalize_json_message(json_objects[0])
                logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON —É—Å–ø–µ—à–Ω–æ: character='{result.get('character')}', content='{result.get('content', '')[:50]}...'")
                return result

        except json.JSONDecodeError as e:
            logger.info(f"   ‚ùå JSON –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è: {e}")

        # Fallback –∫ –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        logger.info("   üìù Fallback –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É")
        result = self._parse_text_format(text)
        logger.info(f"   üìÑ –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥: character='{result.get('character', 'unknown')}', content='{result.get('content', '')[:50]}...'")
        return result

    def _extract_json_objects(self, text: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        json_objects = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON –æ–±—ä–µ–∫—Ç–æ–≤
        pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
        matches = re.finditer(pattern, text, re.DOTALL)

        for match in matches:
            try:
                json_obj = json.loads(match.group())
                json_objects.append(json_obj)
            except json.JSONDecodeError:
                continue

        return json_objects

    def _normalize_json_message(self, data: Dict) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç JSON —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        logger.info(f"üìã –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è JSON: {str(data)[:100]}...")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
        if "messages" in data and isinstance(data["messages"], list):
            # –§–æ—Ä–º–∞—Ç: {"messages": [{"character": "...", "content": "..."}]}
            logger.info("   üìù –ù–∞–π–¥–µ–Ω —Ñ–æ—Ä–º–∞—Ç —Å –º–∞—Å—Å–∏–≤–æ–º messages")
            message = data["messages"][0] if data["messages"] else {}
        else:
            # –§–æ—Ä–º–∞—Ç: {"character": "...", "content": "..."}
            logger.info("   üìù –ù–∞–π–¥–µ–Ω –ø—Ä—è–º–æ–π —Ñ–æ—Ä–º–∞—Ç JSON")
            message = data

        normalized = {
            "character": message.get("character", "unknown"),
            "type": message.get("character_type", message.get("type", "unknown")),
            "mood": message.get("mood", "neutral"),
            "context": message.get("context", "general"),
            "content": message.get("content", message.get("message", "")),
            "timestamp": message.get("timestamp", ""),
            "reply_to": message.get("reply_to"),
            "id": message.get("id", ""),
            "raw_text": json.dumps(message, ensure_ascii=False),
        }
        
        logger.info(f"   ‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: character='{normalized['character']}', type='{normalized['type']}', content='{normalized['content'][:50]}...'")
        return normalized

    def _parse_text_format(self, text: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç (fallback)"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        pattern = r"\[CHARACTER: ([^|]+) \| TYPE: ([^|]+) \| MOOD: ([^|]+) \| CONTEXT: ([^\]]+)\]"
        match = re.search(pattern, text)

        if match:
            character, char_type, mood, context = match.groups()
            content = text[match.end() :].strip()

            return {
                "character": character.strip(),
                "type": char_type.strip(),
                "mood": mood.strip(),
                "context": context.strip(),
                "content": content,
                "raw_text": text,
            }

        return {"content": text, "raw_text": text}

    def parse_all_messages_from_json_array(self, text: str) -> List[Dict]:
        """–ù–æ–≤—ã–π –º–µ—Ç–æ–¥: –ø–∞—Ä—Å–∏—Ç –í–°–ï —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ JSON –º–∞—Å—Å–∏–≤–∞ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        
        –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—è: content, context, reply_to, thread_id –∏ –¥—Ä—É–≥–∏–µ
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ
        """
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ JSON –º–∞—Å—Å–∏–≤–∞: {text[:100]}...")
        
        all_messages = []
        
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
            if text.strip().startswith("{") and text.strip().endswith("}"):
                logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON –æ–±—ä–µ–∫—Ç")
                data = json.loads(text)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–∞—Å—Å–∏–≤ messages –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–∫—Ç–∞
                if "messages" in data and isinstance(data["messages"], list):
                    logger.info(f"   üìù –ù–∞–π–¥–µ–Ω –º–∞—Å—Å–∏–≤ messages —Å {len(data['messages'])} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
                    for i, message in enumerate(data["messages"]):
                        normalized = self._normalize_json_message_extended(message, i)
                        all_messages.append(normalized)
                        logger.info(f"   ‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ {i+1}: {normalized.get('character')} - {normalized.get('content', '')[:50]}...")
                else:
                    # –û–±—ã—á–Ω—ã–π –æ–±—ä–µ–∫—Ç - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
                    normalized = self._normalize_json_message_extended(data, 0)
                    all_messages.append(normalized)
                    logger.info(f"   ‚úÖ –û–¥–∏–Ω–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {normalized.get('character')} - {normalized.get('content', '')[:50]}...")

            # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ JSON –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞–ø—Ä—è–º—É—é
            elif text.strip().startswith("[") and text.strip().endswith("]"):
                logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON –º–∞—Å—Å–∏–≤")
                data = json.loads(text)
                if isinstance(data, list):
                    logger.info(f"   üìù –ù–∞–π–¥–µ–Ω –ø—Ä—è–º–æ–π –º–∞—Å—Å–∏–≤ —Å {len(data)} —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏")
                    for i, message in enumerate(data):
                        normalized = self._normalize_json_message_extended(message, i)
                        all_messages.append(normalized)
                        logger.info(f"   ‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ {i+1}: {normalized.get('character')} - {normalized.get('content', '')[:50]}...")

            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–¥—Ä—è–¥
            else:
                logger.info("   üìù –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞")
                json_objects = self._extract_json_objects(text)
                for i, obj in enumerate(json_objects):
                    normalized = self._normalize_json_message_extended(obj, i)
                    all_messages.append(normalized)
                    logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ {i+1}: {normalized.get('character')} - {normalized.get('content', '')[:50]}...")

        except json.JSONDecodeError as e:
            logger.info(f"   ‚ùå JSON –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –º–µ—Ç–æ–¥—É
            fallback_result = self._parse_text_format(text)
            all_messages.append(fallback_result)

        logger.info(f"   üìä –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(all_messages)}")
        return all_messages

    def _normalize_json_message_extended(self, data: Dict, index: int = 0) -> Dict:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è JSON —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –ø–æ–ª–µ–π
        
        –£—á–∏—Ç—ã–≤–∞–µ—Ç: content, context, reply_to, thread_id, character, mood, timestamp –∏ –¥—Ä.
        """
        logger.info(f"üìã –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è JSON (–∏–Ω–¥–µ–∫—Å {index}): {str(data)[:100]}...")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
        message = data
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è
        normalized = {
            "character": message.get("character", "unknown"),
            "type": message.get("character_type", message.get("type", "unknown")),
            "mood": message.get("mood", "neutral"),
            "context": message.get("context", "general"),
            "content": message.get("content", message.get("message", "")),
            "timestamp": message.get("timestamp", ""),
            "reply_to": message.get("reply_to"),
            "thread_id": message.get("thread_id"),  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
            "id": message.get("id", f"msg_{index:03d}"),
            "raw_text": json.dumps(message, ensure_ascii=False),
            "message_index": index,  # –ò–Ω–¥–µ–∫—Å —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –º–∞—Å—Å–∏–≤–µ
        }
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –≤–∫–ª—é—á–∞—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        extended_content = normalized["content"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        context_parts = []
        if normalized.get("context") and normalized["context"] != "general":
            context_parts.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {normalized['context']}")
        if normalized.get("reply_to"):
            context_parts.append(f"–û—Ç–≤–µ—Ç –Ω–∞: {normalized['reply_to']}")
        if normalized.get("thread_id"):
            context_parts.append(f"–¢–µ–º–∞: {normalized['thread_id']}")
        
        if context_parts:
            extended_content = f"{extended_content}\n[{' | '.join(context_parts)}]"
        
        normalized["extended_content"] = extended_content
        
        logger.info(f"   ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: character='{normalized['character']}', "
                    f"context='{normalized['context']}', thread_id='{normalized.get('thread_id')}', "
                    f"reply_to='{normalized.get('reply_to')}', content='{normalized['content'][:50]}...'")
        
        return normalized

    def get_character_relevant_docs(self, query: str, character: str, top_k: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        logger.info(f"üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}' –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (top_k={top_k})")
        
        if not self.vectorstore:
            logger.warning("‚ùå Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []

        try:
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            character_queries = [
                f'"{character}"',  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏
                f"character: {character}",  # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª—é character
                f"{character} {query}",  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                query,  # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
            ]

            all_docs = []
            character_docs_found = 0
            other_docs_found = 0

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            for query_idx, char_query in enumerate(character_queries):
                logger.info(f"üìù –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {query_idx + 1}/4: '{char_query}'")
                try:
                    docs_with_scores = self.vectorstore.similarity_search_with_score(char_query, k=top_k * 2)
                    logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(docs_with_scores)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç vectorstore")

                    for doc_idx, (doc, score) in enumerate(docs_with_scores):
                        # –ü–∞—Ä—Å–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                        parsed = self.parse_character_message(doc.page_content)
                        parsed_character = parsed.get("character", "unknown").lower().strip()
                        target_character = character.lower().strip()
                        
                        logger.info(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç {doc_idx + 1}: parsed_character='{parsed_character}', target='{target_character}', score={score:.4f}")
                        logger.info(f"      –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {parsed.get('content', '')[:100]}...")
                        
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò - —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
                        if parsed_character == target_character:
                            character_docs_found += 1
                            similarity_score = 1.0 / (1.0 + score)

                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É score - —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                            if similarity_score > 0.3:
                                doc_info = {
                                    "content": parsed.get("content", doc.page_content),
                                    "character": parsed.get("character", "unknown"),
                                    "type": parsed.get("type", "unknown"),
                                    "mood": parsed.get("mood", "neutral"),
                                    "context": parsed.get("context", "general"),
                                    "timestamp": parsed.get("timestamp", ""),
                                    "similarity_score": similarity_score,
                                    "distance_score": float(score),
                                    "metadata": doc.metadata,
                                    "raw_text": doc.page_content,
                                    "query_type": char_query,
                                }
                                all_docs.append(doc_info)
                                logger.info(f"   ‚úÖ –î–û–ë–ê–í–õ–ï–ù –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç {parsed_character} (score={similarity_score:.4f})")
                            else:
                                logger.info(f"   ‚ö†Ô∏è –ü–†–û–ü–£–©–ï–ù –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç {parsed_character} (–Ω–∏–∑–∫–∏–π score={similarity_score:.4f})")
                        else:
                            other_docs_found += 1
                            logger.info(f"   ‚ùå –ü–†–û–ü–£–©–ï–ù –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç '{parsed_character}' (–Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å '{target_character}')")

                except Exception as e:
                    logger.warning(f"‚ùå Query '{char_query}' failed: {e}")
                    continue

            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞: {character_docs_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç {character}, {other_docs_found} –æ—Ç –¥—Ä—É–≥–∏—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π")

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            unique_docs = {}
            for doc in all_docs:
                # key = doc["content"][:100]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∫–ª—é—á
                key = hashlib.md5(doc["content"].encode()).hexdigest()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if key not in unique_docs or doc["similarity_score"] > unique_docs[key]["similarity_score"]:
                    unique_docs[key] = doc

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            character_docs = list(unique_docs.values())
            character_docs.sort(key=lambda x: x["similarity_score"], reverse=True)

            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(character_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for idx, doc in enumerate(character_docs[:5]):  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ª–æ–≥–∞
                logger.info(f"   üìÑ {idx + 1}. {doc['character']} (score={doc['similarity_score']:.4f}): {doc['content'][:80]}...")
            
            return character_docs[:top_k]

        except Exception as e:
            logger.error(f"‚ùå Error getting character documents: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return []

    def get_character_context(self, character: str, mood: Optional[str] = None) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        if character not in self.character_persona.CHARACTERS:
            logger.warning(f"Character {character} not found in persona")
            return ""

        char_info = self.character_persona.CHARACTERS[character]

        context = f"""
            –¢—ã –∏–≥—Ä–∞–µ—à—å —Ä–æ–ª—å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character} –Ω–∞ —Ñ–æ—Ä—É–º–µ.
            –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞:
            - –¢–∏–ø: {char_info['type']}
            - –õ–∏—á–Ω–æ—Å—Ç—å: {char_info['personality']}
            - –°—Ç–∏–ª—å —Ä–µ—á–∏: {char_info['speech_pattern']}
            - –≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞: {char_info['expertise']}
            - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {mood or '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ'}
            –û—Ç–≤–µ—á–∞–π –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ —ç—Ç–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞, –∏—Å–ø–æ–ª—å–∑—É—è –µ–≥–æ —Å—Ç–∏–ª—å —Ä–µ—á–∏ –∏ –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º. –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è. –Ω–µ –±–æ–ª–µ–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ.
            """
        return context

    def validate_json_format(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å JSON —Ñ–æ—Ä–º–∞—Ç–∞ –≤ —Ñ–∞–π–ª–µ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å —Ñ–∞–π–ª –∫–∞–∫ JSON
            try:
                json.loads(content)
                return True
            except json.JSONDecodeError:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ JSON –æ–±—ä–µ–∫—Ç—ã
                json_objects = self._extract_json_objects(content)
                return len(json_objects) > 0

        except Exception as e:
            logger.error(f"Error validating JSON format: {e}")
            return False

    def convert_text_to_json(self, text_content: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ JSON"""
        messages = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
        pattern = r"\[CHARACTER: ([^|]+) \| TYPE: ([^|]+) \| MOOD: ([^|]+) \| CONTEXT: ([^\]]+)\]\s*([^[]*)"
        matches = re.finditer(pattern, text_content, re.MULTILINE | re.DOTALL)

        for i, match in enumerate(matches):
            character, char_type, mood, context, content = match.groups()

            message = {
                "id": f"msg_{i+1:03d}",
                "character": character.strip(),
                "character_type": char_type.strip(),
                "mood": mood.strip(),
                "context": context.strip(),
                "content": content.strip(),
                "timestamp": "",
                "reply_to": None,
            }
            messages.append(message)

        return json.dumps({"messages": messages}, ensure_ascii=False, indent=2)

    def get_character_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –≤ –±–∞–∑–µ"""
        if not self.vectorstore:
            return {}

        stats = {}
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_docs = self.vectorstore.similarity_search("", k=1000)
            logger.info(f"Found {len(all_docs)} documents for character stats")

            for doc in all_docs:
                parsed = self.parse_character_message(doc.page_content)
                character = parsed.get("character", "unknown")

                if character not in stats:
                    stats[character] = {"count": 0, "moods": set(), "contexts": set(), "types": set()}

                stats[character]["count"] += 1
                stats[character]["moods"].add(parsed.get("mood", "neutral"))
                stats[character]["contexts"].add(parsed.get("context", "general"))
                stats[character]["types"].add(parsed.get("type", "unknown"))

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON serialization
            for char in stats:
                stats[char]["moods"] = list(stats[char]["moods"])
                stats[char]["contexts"] = list(stats[char]["contexts"])
                stats[char]["types"] = list(stats[char]["types"])

        except Exception as e:
            logger.error(f"Error getting character stats: {e}")

        return stats

    @timer
    def simulate_forum_discussion(self, topic: str, participants: Optional[List[str]] = None, rounds: int = 3):
        """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ñ–æ—Ä—É–º–Ω—É—é –¥–∏—Å–∫—É—Å—Å–∏—é –º–µ–∂–¥—É –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏"""
        if not participants:
            participants = ["Alaev", "Senior_Dev", "Data_Scientist", "Forum_Moderator"]

        logger.info(f"Starting forum discussion on: {topic}")

        discussion = []
        current_topic = topic

        for round_num in range(rounds):
            logger.info(f"Discussion round {round_num + 1}")

            for participant in participants:
                # –ö–∞–∂–¥—ã–π –ø–µ—Ä—Å–æ–Ω–∞–∂ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ç–µ–∫—É—â—É—é —Ç–µ–º—É
                response = self.ask_as_character(
                    f"–û–±—Å—É–∂–¥–∞–µ–º —Ç–µ–º—É: {current_topic}. –í—ã—Å–∫–∞–∂–∏ —Å–≤–æ–µ –º–Ω–µ–Ω–∏–µ.", participant, translate=True
                )

                discussion.append(
                    {"round": round_num + 1, "character": participant, "message": response, "topic": current_topic}
                )

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–º—É –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞
                current_topic = f"{topic}. –ü—Ä–µ–¥—ã–¥—É—â–∏–π —É—á–∞—Å—Ç–Ω–∏–∫ —Å–∫–∞–∑–∞–ª: {response[:100]}..."

        return discussion

    def get_available_characters(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π"""
        return list(self.character_persona.CHARACTERS.keys())

    def get_character_info(self, character: str) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ"""
        return self.character_persona.CHARACTERS.get(character, {})

    def setup_rag_with_extended_parsing(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç RAG —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON –º–∞—Å—Å–∏–≤–æ–≤
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        –£—á–∏—Ç—ã–≤–∞–µ—Ç: content, context, reply_to, thread_id
        """
        logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ RAG —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON –º–∞—Å—Å–∏–≤–æ–≤...")
        
        # –û—á–∏—â–∞–µ–º –∫–µ—à –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è
        self.clear_cache()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.create_extended_documents_from_json(use_extended_parsing=True)
        
        logger.info("‚úÖ RAG —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    def get_character_relevant_docs_extended(self, query: str, character: str, top_k: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º
        """
        logger.info(f"üîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}' –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (top_k={top_k})")
        
        if not self.vectorstore:
            logger.warning("‚ùå Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []

        try:
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            character_queries = [
                f'"{character}"',  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏
                f"character: {character}",  # –ü–æ–∏—Å–∫ –ø–æ –ø–æ–ª—é character
                f"{character} {query}",  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                query,  # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
            ]

            all_docs = []
            character_docs_found = 0

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            for query_idx, char_query in enumerate(character_queries):
                logger.info(f"üìù –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å {query_idx + 1}/4: '{char_query}'")
                try:
                    docs_with_scores = self.vectorstore.similarity_search_with_score(char_query, k=top_k * 2)
                    logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(docs_with_scores)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç vectorstore")

                    for doc_idx, (doc, score) in enumerate(docs_with_scores):
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ)
                        doc_character = doc.metadata.get('character', 'unknown').lower().strip()
                        target_character = character.lower().strip()
                        
                        # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                        if doc_character == 'unknown':
                            parsed = self.parse_character_message(doc.page_content)
                            doc_character = parsed.get("character", "unknown").lower().strip()
                        
                        logger.info(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç {doc_idx + 1}: character='{doc_character}', target='{target_character}', score={score:.4f}")
                        logger.info(f"      –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {doc.metadata}")
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂—É
                        if doc_character == target_character:
                            character_docs_found += 1
                            similarity_score = 1.0 / (1.0 + score)

                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                            if similarity_score > 0.3:
                                doc_info = {
                                    "content": doc.page_content,
                                    "character": doc_character,
                                    "type": doc.metadata.get('character_type', 'unknown'),
                                    "mood": doc.metadata.get('mood', 'neutral'),
                                    "context": doc.metadata.get('context', 'general'),
                                    "reply_to": doc.metadata.get('reply_to'),
                                    "thread_id": doc.metadata.get('thread_id'),
                                    "timestamp": doc.metadata.get('timestamp', ''),
                                    "message_index": doc.metadata.get('message_index', 0),
                                    "similarity_score": similarity_score,
                                    "distance_score": float(score),
                                    "metadata": doc.metadata,
                                    "raw_text": doc.page_content,
                                    "query_type": char_query,
                                    "extraction_method": doc.metadata.get('extraction_method', 'standard'),
                                }
                                all_docs.append(doc_info)
                                logger.info(f"   ‚úÖ –î–û–ë–ê–í–õ–ï–ù —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç {doc_character} (score={similarity_score:.4f})")
                            else:
                                logger.info(f"   ‚ö†Ô∏è –ü–†–û–ü–£–©–ï–ù –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç {doc_character} (–Ω–∏–∑–∫–∏–π score={similarity_score:.4f})")

                except Exception as e:
                    logger.warning(f"‚ùå Query '{char_query}' failed: {e}")
                    continue

            logger.info(f"üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞: {character_docs_found} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç {character}")

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            unique_docs = {}
            for doc in all_docs:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                key_content = doc["content"]
                key_metadata = f"{doc.get('message_index', 0)}_{doc.get('thread_id', '')}"
                key = hashlib.md5(f"{key_content}_{key_metadata}".encode()).hexdigest()
                
                if key not in unique_docs or doc["similarity_score"] > unique_docs[key]["similarity_score"]:
                    unique_docs[key] = doc

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            character_docs = list(unique_docs.values())
            character_docs.sort(key=lambda x: x["similarity_score"], reverse=True)

            logger.info(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(character_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for idx, doc in enumerate(character_docs[:5]):  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –ª–æ–≥–∞
                logger.info(f"   üìÑ {idx + 1}. {doc['character']} [thread: {doc.get('thread_id', 'N/A')}] (score={doc['similarity_score']:.4f}): {doc['content'][:80]}...")
            
            return character_docs[:top_k]

        except Exception as e:
            logger.error(f"‚ùå Error getting extended character documents: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return []


class ForumManager:

    def __init__(self):
        # ...existing initialization...
        self.forum_rag = ForumRAG("app/ai_manager/forum_knowledge_base", "forum_cache")
        self.character_persona = CharacterPersona()
        self.model = AIModels.gemma

    @timer
    def ask_as_character(
        self, prompt: str,
        character: str,
        mood: Optional[str] = None,
        translate: bool = True,
        extended_docs: bool = True,
            ) -> Union[str, dict, ChatResponse]:
        """–û—Ç–≤–µ—á–∞–µ—Ç –æ—Ç –∏–º–µ–Ω–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        logger.info(f"üé≠ –ó–∞–ø—Ä–æ—Å –æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}' —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ–º '{mood}': {prompt[:100]}...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂
        if character not in self.character_persona.CHARACTERS:
            logger.warning(f"‚ùå –ü–µ—Ä—Å–æ–Ω–∞–∂ '{character}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'alaev' –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            character = "alaev"

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        logger.info(f"üîç –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}'...")
        if extended_docs:
            character_docs = self.forum_rag.get_character_relevant_docs_extended(prompt, character)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥, –µ—Å–ª–∏ extended_docs=False
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            character_docs = self.forum_rag.get_character_relevant_docs(prompt, character)
        
        logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(character_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}'")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        context = ""
        if character_docs:
            context = f"\n–ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π {character}:\n"
            for i, doc in enumerate(character_docs, 1):
                context += f"{i}. [{doc['mood']}] {doc['content'][:200]}...\n"
            context += "\n"
            logger.info(f"üìù –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç({len(context)} —Å–∏–º–≤–æ–ª–æ–≤): {context}")
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}' - –æ—Ç–≤–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ")

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        character_context = self.forum_rag.get_character_context(character, mood)
        logger.info(f"üé≠ –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –¥–ª—è {character}: {character_context[:300]}...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if not prompt.endswith("?"):
            prompt += "?"
        if translate:
            prompt = f"{prompt} (–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.)"

        full_prompt = f"""{character_context}
                        {context}
                        –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: {prompt}
                        –û—Ç–≤–µ—Ç—å –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ {character}.
                        """

        logger.info(f"üìã –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ ({len(full_prompt)} —Å–∏–º–≤–æ–ª–æ–≤): {full_prompt[:300]}...")

        try:
            logger.info(f"ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ {self.model}...")
            response: ChatResponse = chat(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt,
                    },
                ],
            )

            answer = response["message"]["content"]
            logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ '{character}' —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")
            logger.info(f"üí¨ –û—Ç–≤–µ—Ç: {answer[:150]}...")
            return answer

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return f"[{character}] –ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å."

    def save_messages(self, character: str, response: str, question: Optional[str] = None, context: Optional[str] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª answers.txt"""
        try:
            if not question:
                question = "–ë–µ–∑ –≤–æ–ø—Ä–æ—Å–∞"

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message = {
                "character": character,
                "content": response,
                "context": context or "–û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç",
                "timestamp": "",
                "reply_to": None,
                "question": question,
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            with open("answers.txt", "a", encoding="utf-8") as f:
                f.write(json.dumps(message, ensure_ascii=False) + "\n")

            logger.info(f"Message from {character} saved successfully")
        except Exception as e:
            logger.error(f"Error saving message: {e}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    ai_manager = ForumManager() 
    # –ü—Ä–∏–º–µ—Ä 1: –û—Ç–≤–µ—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
    question = """
            –ö–∞–∂–¥—ã–π —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞—é –∂–µ–Ω—â–∏–Ω —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä–∞, –º–∞—à–∏–Ω–∞ , —Ö–æ—Ä–æ—à–æ –æ–¥–µ–≤–∞—é—Ç—Å—è, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –Ω–∏ —É –æ–¥–Ω–æ–π  –Ω–µ—Ç –¥–µ–Ω–µ–≥ –∑–∞–ø–ª–∞—Ç–∏—Ç—å –∑–∞ –µ–¥—É –≤ –∫–∞—Ñ–µ. –ü
            –∞—Ä–∞–¥–æ–∫—Å. –í —á–µ–º –ø—Ä–∏—á–∏–Ω–∞ ? 
                """
    character = "Domen77"
    mood = "sarcastic"
    print(f"üé≠ –û—Ç–≤–µ—Ç –æ—Ç {character}:")
    response = ai_manager.ask_as_character(question, character, mood=mood)     # type: ignore
    # –í—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç
    print(f"{character}: {response["result"]}...")  # type: ignore
    # ai_manager.save_messages(
    #     "Alaev", 
    #     response, 
    #     # context="–û–±—Å—É–∂–¥–µ–Ω–∏–µ –≤–æ–¥–∏—Ç–µ√∑–ª–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∞—Ö"
    #     question=question
    # )
    
    # print("\nüë®‚Äçüíª –û—Ç–≤–µ—Ç –æ—Ç Senior_Dev:")
    # senior_response = ai_manager.ask_as_character(
    #     "–ß—Ç–æ –¥—É–º–∞–µ—à—å –æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º Python?", 
    #     "Senior_Dev", 
    #     mood="professional"
    # )
    # print(f"Senior_Dev: {senior_response}")
    
    # # –ü—Ä–∏–º–µ—Ä 2: –°–∏–º—É–ª—è—Ü–∏—è —Ñ–æ—Ä—É–º–Ω–æ–π –¥–∏—Å–∫—É—Å—Å–∏–∏
    # print("\nüó£Ô∏è –°–∏–º—É–ª—è—Ü–∏—è —Ñ–æ—Ä—É–º–Ω–æ–π –¥–∏—Å–∫—É—Å—Å–∏–∏:")
    # discussion = ai_manager.simulate_forum_discussion(
    #     "–°—Ç–æ–∏—Ç –ª–∏ –∏–∑—É—á–∞—Ç—å Python –≤ 2024 –≥–æ–¥—É?",
    #     participants=["Alaev", "Senior_Dev", "Data_Scientist"],
    #     rounds=2
    # )
    
    # for msg in discussion:
    #     print(f"\n[–†–∞—É–Ω–¥ {msg['round']}] {msg['character']}: {msg['message'][:200]}...")