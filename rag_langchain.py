from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime
import os
import pickle
import json
import hashlib

from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from app.utils.logger_utils import setup_logger, timer

logger = setup_logger(__name__)


class AdvancedRAG:
    def __init__(self, documents_path: str = "knowledge_base", cache_path: str = "cache"):
        self.documents_path = documents_path
        self.cache_path = cache_path
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        self.vectorstore = None
        self.retriever = None
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∫–µ—à–∞
        os.makedirs(self.cache_path, exist_ok=True)
        
        # –§–∞–π–ª—ã –¥–ª—è –∫–µ—à–∞
        self.vectorstore_cache_file = os.path.join(self.cache_path, "vectorstore.pkl")
        self.metadata_cache_file = os.path.join(self.cache_path, "rag_metadata.json")
        
        self.setup_rag()

    def _get_documents_hash(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        hash_md5 = hashlib.md5()
        
        if not os.path.exists(self.documents_path):
            return ""
            
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ  —Ñ–∞–π–ª—ã –∏ –∏—Ö –≤—Ä–µ–º—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
        files_info = []
        for root, dirs, files in os.walk(self.documents_path):
            for filename in sorted(files):
                if filename.endswith(".txt") or filename.endswith(".json"):
                    filepath = os.path.join(root, filename)
                    try:
                        mtime = os.path.getmtime(filepath)
                        size = os.path.getsize(filepath)
                        files_info.append(f"{filename}:{mtime}:{size}")
                    except OSError:
                        continue
        
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–∞—Ö
        files_str = "|".join(files_info)
        hash_md5.update(files_str.encode())
        return hash_md5.hexdigest()

    def _is_cache_valid(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∞–∫—Ç—É–∞–ª–µ–Ω –ª–∏ –∫–µ—à"""
        if not os.path.exists(self.vectorstore_cache_file) or not os.path.exists(self.metadata_cache_file):
            logger.info("–§–∞–π–ª—ã –∫–µ—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return False
        
        try:
            with open(self.metadata_cache_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            current_hash = self._get_documents_hash()
            cached_hash = metadata.get('documents_hash')
            
            if current_hash != cached_hash:
                logger.info("–î–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å, –∫–µ—à —É—Å—Ç–∞—Ä–µ–ª")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if metadata.get('embedding_model') != "sentence-transformers/all-MiniLM-L6-v2":
                logger.info("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –∫–µ—à —É—Å—Ç–∞—Ä–µ–ª")
                return False
            
            logger.info("–ö–µ—à –∞–∫—Ç—É–∞–ª–µ–Ω")
            return True
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–µ—à–∞: {e}")
            return False

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç vectorstore –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫–µ—à"""
        try:
            if self.vectorstore is None:
                logger.warning("–ù–µ—Ç vectorstore –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º vectorstore
            with open(self.vectorstore_cache_file, 'wb') as f:
                pickle.dump(self.vectorstore, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'documents_hash': self._get_documents_hash(),
                'embedding_model': "sentence-transformers/all-MiniLM-L6-v2",
                'documents_count': len(self.vectorstore.docstore._dict) if hasattr(self.vectorstore, 'docstore') else 0,
                'created_at': datetime.now().isoformat(),
                'langchain_version': self._get_langchain_version()
            }
            
            with open(self.metadata_cache_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–ö–µ—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {metadata['documents_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–µ—à–∞: {e}")

    def _load_cache(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç vectorstore –∏–∑ –∫–µ—à–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º vectorstore
            with open(self.vectorstore_cache_file, 'rb') as f:
                self.vectorstore = pickle.load(f)
            
            # –°–æ–∑–¥–∞–µ–º retriever
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            with open(self.metadata_cache_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            logger.info(f"–ö–µ—à –∑–∞–≥—Ä—É–∂–µ–Ω: {metadata.get('documents_count', 0)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {metadata.get('created_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–µ—à–∞: {e}")
            return False

    def _get_langchain_version(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä—Å–∏—é LangChain –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        try:
            import langchain
            return getattr(langchain, '__version__', 'unknown')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–µ—Ä—Å–∏–∏ LangChain: {e}")
            return 'unknown'

    def setup_rag(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ RAG —Å LangChain (—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞–∫—Ç—É–∞–ª–µ–Ω –ª–∏ –∫–µ—à
        if self._is_cache_valid():
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ RAG –∏–∑ –∫–µ—à–∞...")
            if self._load_cache():
                return
            else:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–µ—à, —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ...")
        
        # –°–æ–∑–¥–∞–µ–º RAG –∑–∞–Ω–æ–≤–æ
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ RAG –∏–Ω–¥–µ–∫—Å–∞...")
        self._create_new_rag()

    def _create_new_rag(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π RAG –∏–Ω–¥–µ–∫—Å"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        logger.info(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {self.documents_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        if not os.path.exists(self.documents_path):
            logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.documents_path}")
            return
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
        files_found = []
        for root, dirs, files in os.walk(self.documents_path):
            for file in files:
                if file.endswith(('.txt', '.json')):
                    full_path = os.path.join(root, file)
                    files_found.append(full_path)
                    logger.info(f"   üìÑ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {full_path}")
        
        logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(files_found)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
        documents = []
        for file_path in files_found:
            try:
                logger.info(f"   üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
                loader = TextLoader(file_path, encoding="utf-8")
                file_docs = loader.load()
                documents.extend(file_docs)
                logger.info(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(file_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {file_path}")
            except Exception as e:
                logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                continue

        try:
            if not documents:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ {self.documents_path}")
                logger.info(f"   –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {files_found}")
                return

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            for i, doc in enumerate(documents[:5]):  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                logger.info(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç {i+1}: source={doc.metadata.get('source', 'unknown')}, length={len(doc.page_content)}")
                logger.info(f"      –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {doc.page_content[:100]}...")

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            logger.info("‚úÇÔ∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=800,  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è JSON —Ñ–∞–π–ª–æ–≤ –Ω–∞ —á–∞—Å—Ç–∏
                chunk_overlap=100,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                length_function=len
            )

            texts = text_splitter.split_documents(documents)
            logger.info(f"üìù –°–æ–∑–¥–∞–Ω–æ {len(texts)} —á–∞–Ω–∫–æ–≤")

            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            logger.info("üß† –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
            self.vectorstore = FAISS.from_documents(texts, self.embeddings)

            # –°–æ–∑–¥–∞–µ–º retriever
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity", 
                search_kwargs={"k": 3}
            )
            
            logger.info("‚úÖ RAG —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            self._save_cache()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ RAG: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            self.vectorstore = None
            self.retriever = None

    def get_relevant_docs(self, query: str, method: str = "adaptive") -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        logger.info(f"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query} (method={method})")
        
        if not self.retriever:
            logger.warning("Retriever –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É RAG.")
            return []

        try:
            if method == "adaptive":
                docs_info = self.get_adaptive_relevant_docs(query)
            elif method == "filtered":
                docs_info = self.get_relevant_docs_filtered(query)
                docs_info = [{'content': doc} for doc in docs_info]  # Normalize format
            elif method == "ranked":
                docs_info = self.get_ranked_relevant_docs(query)
            elif method == "contextual":
                docs_info = self.get_contextual_relevant_docs(query)
            else:
                # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –º–µ—Ç–æ–¥—É
                docs = self.retriever.get_relevant_documents(query)
                result = [doc.page_content for doc in docs]
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥)")
                return result
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            result = [doc_info['content'] for doc_info in docs_info]
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—á–µ—Å—Ç–≤–µ
            for i, doc_info in enumerate(docs_info):
                similarity = doc_info.get('similarity_score', 0)
                source = doc_info.get('source', 'unknown')
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {i+1}: similarity={similarity:.4f}, source={source}")
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({method} –º–µ—Ç–æ–¥)")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_relevant_docs_with_metadata(self, query: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        logger.info(f"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
        
        if not self.retriever:
            logger.warning("Retriever –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É RAG.")
            return []

        try:
            docs = self.retriever.get_relevant_documents(query)
            result = []
            
            for doc in docs:
                doc_info = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'source': doc.metadata.get('source', 'unknown')
                }
                result.append(doc_info)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_relevant_docs_filtered(self, query: str, similarity_threshold: float = 0.7) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ similarity score"""
        logger.info(f"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
        
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=10)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ threshold
            filtered_docs = []
            for doc, score in docs_with_scores:
                # –ß–µ–º –º–µ–Ω—å—à–µ score, —Ç–µ–º –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–µ—Å—Ç—å –≤ FAISS
                if score <= similarity_threshold:
                    filtered_docs.append(doc.page_content)
                    logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏–Ω—è—Ç: score={score:.4f}")
                else:
                    logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω: score={score:.4f}")
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (threshold={similarity_threshold})")
            return filtered_docs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_ranked_relevant_docs(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ similarity score"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k * 2)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            ranked_docs = []
            for doc, score in docs_with_scores:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score (–ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ similarity score –æ—Ç 0 –¥–æ 1)
                similarity_score = 1.0 / (1.0 + score)
                
                doc_info = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': similarity_score,
                    'distance_score': float(score),
                    'source': doc.metadata.get('source', 'unknown'),
                    'relevance_rank': len(ranked_docs) + 1
                }
                ranked_docs.append(doc_info)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ similarity score (—É–±—ã–≤–∞–Ω–∏–µ)
            ranked_docs.sort(key=lambda x: x['similarity_score'], reverse=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–Ω–≥–∏ –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            for i, doc in enumerate(ranked_docs):
                doc['relevance_rank'] = i + 1
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ top_k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            result = ranked_docs[:top_k]
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫–µ—à"""
        cache_files = [self.vectorstore_cache_file, self.metadata_cache_file]
        
        for cache_file in cache_files:
            if os.path.exists(cache_file):
                os.remove(cache_file)
                logger.info(f"–£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –∫–µ—à–∞: {cache_file}")
        
        logger.info("–ö–µ—à –æ—á–∏—â–µ–Ω")

    def rebuild_index(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å"""
        logger.info("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
        self.clear_cache()
        self._create_new_rag()

    def get_cache_info(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–µ—à–µ"""
        cache_info = {
            'cache_exists': self._is_cache_valid(),
            'cache_files': {},
            'documents_path': self.documents_path,
            'cache_path': self.cache_path
        }
        
        cache_files = [
            ('vectorstore', self.vectorstore_cache_file),
            ('metadata', self.metadata_cache_file)
        ]
        
        for name, filepath in cache_files:
            if os.path.exists(filepath):
                stat = os.stat(filepath)
                cache_info['cache_files'][name] = {
                    'size_mb': round(stat.st_size / 1024 / 1024, 2),
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if os.path.exists(self.metadata_cache_file):
            try:
                with open(self.metadata_cache_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                cache_info['metadata'] = metadata
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
        
        return cache_info

    def add_documents(self, new_documents: List[str], sources: Optional[List[str]] = None):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            from langchain.schema import Document
            
            docs_to_add = []
            for i, doc_text in enumerate(new_documents):
                source = sources[i] if sources and i < len(sources) else f"added_doc_{i}"
                doc = Document(
                    page_content=doc_text,
                    metadata={"source": source, "added_at": datetime.now().isoformat()}
                )
                docs_to_add.append(doc)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500, 
                chunk_overlap=50, 
                length_function=len
            )
            
            texts = text_splitter.split_documents(docs_to_add)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ vectorstore
            self.vectorstore.add_documents(texts)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º retriever
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity", 
                search_kwargs={"k": 3}
            )
            
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(texts)} –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫–µ—à
            self._save_cache()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")

    def get_documents_count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ"""
        if not self.vectorstore or not hasattr(self.vectorstore, 'docstore'):
            return 0
        return len(self.vectorstore.docstore._dict)

    def search_similar_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º similarity_search_with_score –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k)
            
            result = []
            for doc, score in docs_with_scores:
                doc_info = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': float(score),
                    'source': doc.metadata.get('source', 'unknown')
                }
                result.append(doc_info)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(result)} –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def get_adaptive_relevant_docs(self, query: str, max_docs: int = 5, quality_threshold: float = 0.6) -> List[Dict]:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ similarity"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=max_docs * 3)
            
            if not docs_with_scores:
                return []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            scores = [score for _, score in docs_with_scores]
            avg_score = sum(scores) / len(scores)
            min_score = min(scores)
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π threshold –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if min_score < 0.3:  # –û—á–µ–Ω—å —Ö–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                threshold = avg_score * 0.8
            elif min_score < 0.6:  # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                threshold = avg_score * 0.9
            else:  # –°–ª–∞–±–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                threshold = avg_score * 1.1
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            filtered_docs = []
            for doc, score in docs_with_scores:
                if score <= threshold and len(filtered_docs) < max_docs:
                    similarity_score = 1.0 / (1.0 + score)
                    
                    doc_info = {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'similarity_score': similarity_score,
                        'distance_score': float(score),
                        'source': doc.metadata.get('source', 'unknown'),
                        'quality_level': self._assess_quality(similarity_score)
                    }
                    filtered_docs.append(doc_info)
            
            logger.info(f"–ê–¥–∞–ø—Ç–∏–≤–Ω–æ –Ω–∞–π–¥–µ–Ω–æ {len(filtered_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (threshold={threshold:.4f})")
            return filtered_docs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def get_contextual_relevant_docs(self, query: str, context_window: int = 2) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤"""
        if not self.vectorstore:
            logger.warning("Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=5)
            
            enhanced_docs = []
            for doc, score in docs_with_scores:
                similarity_score = 1.0 / (1.0 + score)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–≥–æ –∂–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source = doc.metadata.get('source', '')
                context_docs = self._find_context_chunks(source, doc.page_content, context_window)
                
                doc_info = {
                    'content': doc.page_content,
                    'context_content': context_docs,
                    'metadata': doc.metadata,
                    'similarity_score': similarity_score,
                    'distance_score': float(score),
                    'source': source,
                    'has_context': len(context_docs) > 0
                }
                enhanced_docs.append(doc_info)
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(enhanced_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
            return enhanced_docs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {e}")
            return []

    def _find_context_chunks(self, source: str, main_content: str, window: int) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏ –∏–∑ —Ç–æ–≥–æ –∂–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if not self.vectorstore:
            return []
        
        try:
            # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–æ–≥–æ –∂–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_docs = self.vectorstore.similarity_search(f"source:{source}", k=20)
            
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤
            context_chunks = []
            for doc in source_docs:
                if (doc.metadata.get('source') == source and 
                    doc.page_content != main_content and 
                    len(context_chunks) < window):
                    context_chunks.append(doc.page_content)
            
            return context_chunks
            
        except Exception as e:
            logger.info(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return []

    def _assess_quality(self, similarity_score: float) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        if similarity_score > 0.8:
            return "excellent"
        elif similarity_score > 0.6:
            return "good"
        elif similarity_score > 0.4:
            return "fair"
        else:
            return "poor"

    def create_extended_documents_from_json(self, use_extended_parsing: bool = True) -> None:
        """–°–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON –º–∞—Å—Å–∏–≤–æ–≤
        
        –í–º–µ—Å—Ç–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏, –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ JSON —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        –£—á–∏—Ç—ã–≤–∞–µ—Ç: content, context, reply_to, thread_id
        """
        logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON –º–∞—Å—Å–∏–≤–æ–≤...")
        
        if not os.path.exists(self.documents_path):
            logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.documents_path}")
            return
        
        # –ù–∞–π–¥–µ–º –≤—Å–µ JSON —Ñ–∞–π–ª—ã
        json_files = []
        for root, dirs, files in os.walk(self.documents_path):
            for file in files:
                if file.endswith('.json'):
                    full_path = os.path.join(root, file)
                    json_files.append(full_path)
                    logger.info(f"   üìÑ –ù–∞–π–¥–µ–Ω JSON —Ñ–∞–π–ª: {full_path}")
        
        logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(json_files)} JSON —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        try:
            from langchain.schema import Document
            
            all_documents = []
            
            for file_path in json_files:
                logger.info(f"   üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
                    if hasattr(self, 'parse_all_messages_from_json_array'):
                        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ ForumRAG, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                        messages = self._parse_json_messages_extended(content, file_path)
                    else:
                        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É
                        messages = self._simple_json_parse(content, file_path)
                    
                    logger.info(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ {file_path}")
                    
                    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                    for i, message in enumerate(messages):
                        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                        document_content = self._create_extended_content(message)
                        
                        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        metadata = {
                            'source': file_path,
                            'message_index': i,
                            'character': message.get('character', 'unknown'),
                            'thread_id': message.get('thread_id'),
                            'reply_to': message.get('reply_to'),
                            'context': message.get('context', 'general'),
                            'mood': message.get('mood', 'neutral'),
                            'timestamp': message.get('timestamp', ''),
                            'extraction_method': 'extended_json_parsing'
                        }
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                        doc = Document(
                            page_content=document_content,
                            metadata=metadata
                        )
                        all_documents.append(doc)
                        
                        logger.info(f"      üìÑ –°–æ–∑–¥–∞–Ω –¥–æ–∫—É–º–µ–Ω—Ç {i+1}: {message.get('character')} - {message.get('content', '')[:50]}...")

                except Exception as e:
                    logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    continue
            
            if not all_documents:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–æ–≤")
                return
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_documents)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ë–ï–ó –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
            logger.info("üß† –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏...")
            self.vectorstore = FAISS.from_documents(all_documents, self.embeddings)
            
            # –°–æ–∑–¥–∞–µ–º retriever
            self.retriever = self.vectorstore.as_retriever(
                search_type="similarity", 
                search_kwargs={"k": 5}  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º k –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —á–∏—Å–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            )
            
            logger.info("‚úÖ RAG —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            self._save_cache()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            self.vectorstore = None
            self.retriever = None

    def _parse_json_messages_extended(self, content: str, source_file: str) -> List[Dict]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        messages = []
        
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON
            if content.strip().startswith("{") and content.strip().endswith("}"):
                data = json.loads(content)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–∞—Å—Å–∏–≤ messages
                if "messages" in data and isinstance(data["messages"], list):
                    for i, message in enumerate(data["messages"]):
                        normalized = self._normalize_message(message, i, source_file)
                        messages.append(normalized)
                else:
                    # –û–¥–∏–Ω–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                    normalized = self._normalize_message(data, 0, source_file)
                    messages.append(normalized)
                    
            elif content.strip().startswith("[") and content.strip().endswith("]"):
                # –ü—Ä—è–º–æ–π –º–∞—Å—Å–∏–≤
                data = json.loads(content)
                if isinstance(data, list):
                    for i, message in enumerate(data):
                        normalized = self._normalize_message(message, i, source_file)
                        messages.append(normalized)
            
        except json.JSONDecodeError as e:
            logger.info(f"JSON parsing failed for {source_file}: {e}")
            # Fallback –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É
            messages.append({
                'character': 'unknown',
                'content': content,
                'context': 'general',
                'mood': 'neutral',
                'reply_to': None,
                'thread_id': None,
                'timestamp': '',
                'message_index': 0
            })
        
        return messages

    def _simple_json_parse(self, content: str, source_file: str) -> List[Dict]:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –ø–∞—Ä—Å–∏–Ω–≥ JSON"""
        try:
            data = json.loads(content)
            if isinstance(data, dict) and "messages" in data:
                return data["messages"]
            elif isinstance(data, list):
                return data
            else:
                return [data]
        except:
            return [{'content': content, 'character': 'unknown'}]

    def _normalize_message(self, message: Dict, index: int, source_file: str) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        return {
            'character': message.get('character', 'unknown'),
            'content': message.get('content', ''),
            'context': message.get('context', 'general'),
            'mood': message.get('mood', 'neutral'),
            'reply_to': message.get('reply_to'),
            'thread_id': message.get('thread_id'),
            'timestamp': message.get('timestamp', ''),
            'character_type': message.get('character_type', 'unknown'),
            'message_index': index,
            'source_file': source_file
        }

    def _create_extended_content(self, message: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
        content = message.get('content', '')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        context_parts = []
        
        if message.get('context') and message['context'] != 'general':
            context_parts.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {message['context']}")
        
        if message.get('reply_to'):
            context_parts.append(f"–û—Ç–≤–µ—Ç –¥–ª—è: {message['reply_to']}")
        
        if message.get('thread_id'):
            context_parts.append(f"–¢–µ–º–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è: {message['thread_id']}")
        
        if message.get('mood') and message['mood'] != 'neutral':
            context_parts.append(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {message['mood']}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        if context_parts:
            extended_content = f"{content}\n\n[{' | '.join(context_parts)}]"
        else:
            extended_content = content
        
        return extended_content
